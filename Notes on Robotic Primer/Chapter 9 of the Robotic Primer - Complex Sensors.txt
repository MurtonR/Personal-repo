
Ultrasonic or Sonar Sensing:

Ultrasound means "beyond sound" and refers to a range of frequencies that cannot be heard by humans. It is also called sonar (So(sound), na(navigation), r(ranging).

Using ultrasound/sonar in order to determine a robot's position is called echolocation and works by producing a sound that bounces off of surfaces and forms echoes that are used to find one's place in an environment.

In nature there are many examples of echolocation, for example bats or dolphins. These use it for avoiding collision, hunting and various other tasks, such as in dolphins navigating mazes (in aquariums). Synthetic sensors that use sonar are no where near as sophisticated as biological sensors, however synthetic sensors still find a use in robotics and other scientific fields.


Artifical sensors work off the 'time-of-flight' principle, meaning they measure the time it takes for sound to travel(fly). Sonars are active sensors that function by having an emitter that produces an ultrasonic chirp or ping that travels away from the emitter until it meets a barrier in which case it may reflect back to the detector, however if there are no barriers then the sonar ping does not return, the sound wave also weakens (attenuates) with distance and eventually breaks down.

Each time a sonar ping is emmited a timer is started, this timer is stopped if it has been too long and the sound wave hasn't returned or if it returns to the sensor. If it returns then the time it took to come back is divided by the speed of sound and then by 2, as we only want the time it takes for the wave to reach the barrier.

This is a very simple computation as it only relies on knowing the speed of sound, which is only effected by the ambient temperatue, at room temperature it is 1.12 feet per millisecond, or 0.89 seconds to travel 1 foot, this is important to remember.

The most commonly used hardware for sonar sensors in robotics is the polaroid ultrasound sensor, which was intitally designed for instant cameras. The sensor consists on one round 'transducer' approximately 1 inch in diameter that emits a chirp or ping and recieves the echo.

A 'transducer' is a device that converts one kind of energy into another, in this case it is converting mechanical energy into sound. It does this by flexing (mechanical) a membrane that produces a ping (sound) that sends out a sound wave that is inaudible to humans.

It is possible for humans to hear a clicking from a polaroid ultrasound sensor as it senses, however this is not the ultrasound wave but instead the inital flexing that produces the audible ping.


The hardware (electronics) of a polaroid ultrasound sensor uses relative high power due to the significant current required for each ping. The current used by these sensors is larger than the usage of a computer's processors.

This is another reason for why it is important to seperate the power electronics of a robot's sensing and actuating mechanisms from those of it's controller processor, or else the robot's brain may have to slow down the processsor when  sensing or moving.

The polarois ultrasound sensor emits it's ultrasound waves in 30 degree cones in all directions, it also has a base attenuation of 32 feet where they will no longer returns. This is satisfactory for most uses in robotics especially as robots are commonly used in indoor environments where spaces will rarely be larger than 32 feet in a single direction. The attentuation of a sound wave can be decreased or increased depending on the environment and goals that a robot has to be optimised for.



Sonar before and beyond robotics:

Ultrasound is commonly used in medicine for checking in on babies in utero for example. The polaroid and other sonars in robotics operate at 50KHz whereas those used in medicine commonly have a frequency of 3.5 to 7MHz.

A Hertz (Hz) is a unit of frequency and 1hz means once a second. 

In robotics simple sensing distance is enough however in medical use much more complex post processing is required in order to create a composite image of a body part that updates in real time for example the beating heart of a babie in utero


Since sound travels very well in water while vision is almost useless it is obvious why sonar is used in submarines in order to navigate without crashing into the ocean floor or another submarine. Sonar signals in submarines use harsher cones and stronger signal intensity in order to be able to reach further than 32 feet.

There are other uses of ultrasound waves other than their use in sonar for good and bad, for example high stength and long range sonar signals have been observed to confuse whales and cause them to beach and die. Other uses of ultrasound are; to break up kidney stones within the body, burglary alarms, height measures and create heat to weld plastics.

In almost uses multiple sonar sensors are employed in order to maximise coverage and accuracy, most robots that use sonar are equipped with a full ring of sensors in order to increase the coverage and accuracy of any readings. However not all the sensors are able to be pinged at once.

Sonar sensors are relatively inexpensive and easy to incoporate into robot hardware, however sonar sensors do not always relay accureate distance readings.


Specular Readings:

The accuracy of sonar reading rely on the emitted sound wave returning to the sensor by reflecting off of surfaces, however the direction of the reflection of a sound wave from a surface relies upon multiple factors; the properties of the surface(how smooth it is), the incident angle of the sound beam and the surface(how sharp the surface is).

One major disadvantage of sonar sensing is it's susceptibility to specular reflection.

'Specular reflection' is the reflection from the outer side of the object, in this case it means the sound wave reflecting off of multiple surfaces before it returns to the sensor, this can lead to false longer distance readings that are longer than the straight line distance from the object to the sensor. This most commonly occurs if the first surface that the sound wave encounters is smooth, if angle between the beam ans surface is small. 

If the angle is small then there is an increased chance that the beam will simply graxe and bounce off without returning to the sensor, there is also the chance that the wave reflects from more surfaces after befor returning leading to an incorrect reading. 

If the surface is smooth there is a higher chance that the beam will reflect in a different direction away from the sensor. If the surface is rough then the sound wave will have irregular reflections, as the reflections are at more varied angles there is higher chance that some of the reflections will return to the sensor and provide a decently accurate distance reading.

In the worst cases of specular reflection the sound wave reflects from multiple surfaces and never returns to the sensor, fooling the robot into believing there are no barriers there, which could cause the robot to collide into the barrier.
The other case is when the sensor returns a far away reading when the barrier is close which has the same issues as the first case. This is more likely to happen in environments such as a room of mirrors, a carnival or the glass cases at a museum. 

A crude but effective way to combat specular reflection is to modify the surfaces around the environment that the robot will sense. This can be done by making them less smooth by using sandpaper to make them less smooth or attaching wood slats, the key idea is that we are adding features to the surfaces to make specular reflection less of an issue.

Really only the surfaces that are able to be within a cone of a sonar sensor need to be altered but this method is not convient or useful in real world applications, as it is effectively impossible to ensure that every surface that a robot will sense in real life applications is rough or has many features.

One solution to this problem that doesn't involve having to change the environment of the robot is to have multiple sonar sensors that cover the same area and are made to activate out of phase, this provides multiple readings for the same area and as such increases accuracy.


We can make the robot process long distance readings more than shorter distance readings in order to try an eliminate incorrect readings due to specular reflection. One idea is to keep a history of past readings and see if they increase or decrease in a continous, reasonable way. If they do the robot accepts them, if they don't then they are assumed to be due to specular reflection and are ignored.

However this idea does have flaws, if the environment has large 'discontinuities' (sudden and large changes in its features) this causes the robot to think that real sudden long distance readings are from speculative reflections, causing the robot to misidentify it's environment. These changes cannot be predicted by the robot and as such have to be left up to the robots own logic to identify the false readings.


The robot can use action-orientated perception in this case for example when the robot recieves reading which it believs are suspicous or strange it can change the angle between its sonar sensor and the environment and position in order to obtain more readings that are hopefully more accurate.



Laser Sensing:

Laser sensors are largely unaffected by specular reflection and this makes them more useful in certain circumstances.

Lasers emit highly amplified and coherent radiation at one or more frequencies. The lasers may be in the visble spectrum but they also might not, it all depends on their use. For example the lasers used in burglar alarms usually use lasers not in the visible spectrum as it makes a would be burglar's job more difficult. A burglar alarms is also commonly an example of a laser break beam sensor as it is only once the beam is broken that the alarms is triggered. Additionally when lasers are used for measuring in robots they are usually not in the visible spectrum as it is usually not desirable to have a distracting beam of light scanning the environment as the robot navigates. 	

Lasers can also use the 'time-of-flight' principle, the are significantly faster than sonar counterparts due to the speed of light being faster than that of sound. However this can cause issues when measuring short distances. The light travel to and fro to quick to be measured by modern day electonics, so instead phase-shift measurements are used.

Lasers are bigger and much higher powered than sonar sensors however they are also much more accurate.

Lasers also use beams instead of cones, the beams are usually 3mm in diameter. Because lasers use light they can take many more measurements than sonar and as such can provide data with a higher 'resolution'. 'Resolution' refers to the process of seperating or breaking something into constituent parts. As such a higher resolution literally more parts and the more parts there are the more information, this is why a higher resolution is good.

Lasers sound perfect however lasers have downsides. One such downside is the size, they are large with many laser sensors being the size of an electric coffee maker, and they cost significantly more than sonar sensors. Additionally if a laser sensor needs to scan a larger area outside of the usual 180 degree horizontal scan it requires even more sensing and processing time

3D laser scanners do exist however they are larger and more expensive than laser sensors. They are ideal for mapping out distances to objects thus the space around the sensor (or the robot using the sensor).


For small or mobile robots simple sensors are preferred and lasers and not portable or affordable enough to be common.

Some scenarios where lasers are not preferrable are:

-Small robots

-Robots that are going to be interacting with children

-Robots that are going to be interacting with people who may look in the direction of the laser


Visual Sensing:

The field that deals with vision in machinesm including robots is called 'machine vision'.

Machine vision and robotics are seperate fields with some overlapping intrests, problems and uses.


Traditionally machine vision has concerned itself with answering questions similiar to "what is that?", "who is that", "where is that", The approach to answer these questions has been to reconstruct what the world was like when the photo was taken.

Luckily for robotics these aren't the questions we need to answer, as robots are usually concerned with doing the right thing to achieve their goals. So robots need to answer questions like "should I keep going or turn or stop?", "should I grab or let go?" or "where and when do I turn?". It is not usually necessary to reconstruct the world to answer these questions.




Cameras:

Cameras are biomemetic in that they function some what how eyes do naturally, however there are some key differences between the two.

Here are the key components:

-Light scattered from objects in the environment (collectively the 'scene') goes through the iris.

-The 'iris' at its simpliest is just a simple pinhole however usually it is a lens and hits the image plane.

-The image plane corresponds to the retina of the biological eye which is attached to rods and cones.

- Rods and cones are numerous light sensitive elements (photosensitive) that are attached to nerves that perform early                            vision

-Early vision is the first stage of visual image processing and then passes information to the parts of the brain to perform high level vision processing


If a 'lens' is used then more light can get in but at the cost of being focused, and only objects at a particular range of distances from the lens will be in focus, this range of distances is called the camera's 'depth of field'.

The image plane is usually subdivided into equal parts called 'pixels' typically arranged in a rectangular grid. On a typical camera the grib is 512 x 512 pixels, compared to the 120 × 106 rods and 6 × 106 cones in the human eye it seems like not very many.

This projection of the scene on the image plane is called the 'image'. The brightness of each pixel is proportional to the amount of light that was reflected from the object or surface that projects to that pixel, called the surface patch.

There are two types of reflection; specular (off the surface) and 'diffuse' reflections.'Diffuse' reflection consists of light that penetrates into the object, is absorbed and then comes back out.

Like the human eye, cameras percieve the world consistently and as such it captures a video which is a series of images over time. 

Processing any 'time' series information over time is pretty complicated. In machine vision each individual snapshot over time is called a 'frame'. Getting frames out of a time series is not simple and requires a 'frame grabber', which is a piece of specialised hardware that captures a single frame from a videos analog signal and stores it as a 'digital image'. Finally we can move onto the next step of visual processing called 'image processing'.